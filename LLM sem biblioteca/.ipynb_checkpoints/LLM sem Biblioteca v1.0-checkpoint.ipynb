{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61655b7f-24ee-47b7-a4b1-20a5c95a24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\nEste', '\\nUse', '(@#%)', '(á,', '1:', 'DOCUMENTO', 'Introdução', 'MOCK\\nCapítulo', 'Você.', 'acentos', 'com', 'e', 'embeddings!\\nAutor:', 'fictício', 'para', 'seu', 'símbolos', 'testar', 'texto', 'tokenizador', 'um', 'ã,', 'ç)', 'é', 'é,']\n"
     ]
    }
   ],
   "source": [
    "#PASSO 1: Tokenização\n",
    "#Aqui a gente transforma o texto em números\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Aqui abaixo na variável texto precisa fazer a injeção de textos das fontes que quiser.\n",
    "texto = \"DOCUMENTO MOCK\\nCapítulo 1: Introdução  \\nEste é um texto fictício com acentos (á, é, ã, ç) e símbolos (@#%) \\nUse para testar seu tokenizador e embeddings!\\nAutor: Você.\"\n",
    "texto = texto.split(' ')\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "print(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c3450f1-e0e3-44e2-8a17-c5533cc3cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '\\nEste': 1, '\\nUse': 2, '(@#%)': 3, '(á,': 4, '1:': 5, 'DOCUMENTO': 6, 'Introdução': 7, 'MOCK\\nCapítulo': 8, 'Você.': 9, 'acentos': 10, 'com': 11, 'e': 12, 'embeddings!\\nAutor:': 13, 'fictício': 14, 'para': 15, 'seu': 16, 'símbolos': 17, 'testar': 18, 'texto': 19, 'tokenizador': 20, 'um': 21, 'ã,': 22, 'ç)': 23, 'é': 24, 'é,': 25}\n",
      "{0: '', 1: '\\nEste', 2: '\\nUse', 3: '(@#%)', 4: '(á,', 5: '1:', 6: 'DOCUMENTO', 7: 'Introdução', 8: 'MOCK\\nCapítulo', 9: 'Você.', 10: 'acentos', 11: 'com', 12: 'e', 13: 'embeddings!\\nAutor:', 14: 'fictício', 15: 'para', 16: 'seu', 17: 'símbolos', 18: 'testar', 19: 'texto', 20: 'tokenizador', 21: 'um', 22: 'ã,', 23: 'ç)', 24: 'é', 25: 'é,'}\n"
     ]
    }
   ],
   "source": [
    "#Seta cada palavra a um token e vice versa\n",
    "token_para_id = {c: i for i, c in enumerate (vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate (vocabulario)}\n",
    "print(token_para_id)\n",
    "print(id_para_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31967980-37d4-479b-9da5-df6752369b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [6, 8, 5, 7, 0, 1, 24, 21, 19, 14, 11, 10, 4, 25, 22, 23, 12, 17, 3, 2, 15, 18, 16, 20, 12, 13, 9]\n"
     ]
    }
   ],
   "source": [
    "#Tokenização\n",
    "tokens = [token_para_id[c] for c in texto] #Reescreve o texto inicial como tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32a255c-6e44-4f07-a6cb-32fd6087486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.53388905e+00 -7.01273251e-01  6.44209165e-02 ... -7.50096106e-02\n",
      "  -5.85338877e-01 -1.58423832e+00]\n",
      " [-7.65559703e-01  6.22283399e-01 -1.06689780e+00 ... -1.78576723e-01\n",
      "  -1.31392562e+00  6.39644941e-01]\n",
      " [ 1.12000406e+00  8.67205839e-01 -6.06377310e-01 ...  7.36887330e-01\n",
      "   1.36540736e+00  4.33275282e-02]\n",
      " ...\n",
      " [ 4.03254377e-01  6.93741757e-01  8.98599557e-01 ... -1.84473342e-01\n",
      "  -5.03970670e-01 -8.11775638e-01]\n",
      " [ 1.09754040e-01 -4.05250301e-04  1.10381927e+00 ... -9.53911738e-01\n",
      "   5.79168250e-01 -1.50952660e+00]\n",
      " [-1.87312531e+00  7.24128148e-01  1.51773307e+00 ...  1.18599838e+00\n",
      "  -6.14265264e-01 -7.47586464e-02]]\n"
     ]
    }
   ],
   "source": [
    "#PASSO 2: Embedding (Vetores de Palavras)\n",
    "#Cada token vira um vetor de dimensão \"d_model\"\n",
    "\n",
    "d_model = 256 #Isso é o tanto de números que vão representar uma letra! (Sim, precisa ser um número razoavelmente alto porque precisa diferenciar acentuações, maiúsculas, minúsculas e todo tipo de variação que existir de um caractere para outro)\n",
    "vocab_size = len(vocabulario)\n",
    "\n",
    "#Embeddings aleatórios (normalmente aprendidos durante o treino)\n",
    "embedding = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "#Transforma tokens em vetores\n",
    "token_embeddings = embedding[tokens] #Shape: [len(tokens), d_model]\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fbc4809-05ce-4b6d-95c2-e46839a7a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASSO 3: Mecanismo de Atenção (Self-Attention)\n",
    "#Calcula a relação entre tokens, versão simplificada:\n",
    "def attention(Q, K, V):\n",
    "    # Q, K, V são matrizes de consulta, chave e valor\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Similaridade: Calcula o \"quão bem\" a palavra Q se relaciona com as outras (K)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax: Transforma os scores em probabilidades (soma = 1)\n",
    "    return np.dot(weights, V)  # Combinação: Combina os embeddings das palavras (V) usando os pesos de atenção\n",
    "\n",
    "# Exemplo:\n",
    "Q = token_embeddings  # Consulta: O que cada palavra está buscando\n",
    "K = token_embeddings  # Chave: Como cada palavra se descreve\n",
    "V = token_embeddings  # Valor: Informação real de cada palavra\n",
    "saida_attention = attention(Q, K, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12dfacb3-3237-4cd5-9ca0-837277ecd6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.53388777e+00 -7.01272922e-01  6.44214782e-02 ... -7.50096494e-02\n",
      "  -5.85338311e-01 -1.58423735e+00]\n",
      " [-7.65543705e-01  6.22274567e-01 -1.06687341e+00 ... -1.78571089e-01\n",
      "  -1.31390217e+00  6.39626477e-01]\n",
      " [ 1.12000132e+00  8.67203695e-01 -6.06375338e-01 ...  7.36885389e-01\n",
      "   1.36540477e+00  4.33270928e-02]\n",
      " ...\n",
      " [ 4.03253865e-01  6.93741406e-01  8.98599336e-01 ... -1.84473252e-01\n",
      "  -5.03970549e-01 -8.11775289e-01]\n",
      " [ 1.09753787e-01 -4.05347109e-04  1.10381803e+00 ... -9.53911225e-01\n",
      "   5.79167899e-01 -1.50952531e+00]\n",
      " [-1.87312509e+00  7.24128077e-01  1.51773293e+00 ...  1.18599819e+00\n",
      "  -6.14265193e-01 -7.47587005e-02]]\n"
     ]
    }
   ],
   "source": [
    "#PASSO 4: Rede Neural Feed-Forward\n",
    "#Uma MLP simples para processar os vetores após o mecanismo de atenção\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    hidden = np.maximum(0, np.dot(x, W1) + b1)  # Camada oculta com ReLU\n",
    "    return np.dot(hidden, W2) + b2 # Camada de saída\n",
    "\n",
    "# Pesos aleatórios (em um modelo real, seriam aprendidos)\n",
    "W1 = np.random.randn(d_model, 4 * d_model) # Pesos da camada oculta\n",
    "b1 = np.random.randn(4 * d_model) # Vieses da camada oculta\n",
    "W2 = np.random.randn(4 * d_model, d_model) # Pesos da camada de saída\n",
    "b2 = np.random.randn(d_model) # Vieses da camada de saída\n",
    "\n",
    "saida_ff = feed_forward(saida_attention, W1, b1, W2, b2)\n",
    "\n",
    "print(saida_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e35691f-0e41-418e-8a66-07a17b838492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.0487712919645915\n"
     ]
    }
   ],
   "source": [
    "#PASSO 5: Treinamento\n",
    "#Para treinar precisaríamos\n",
    "#1 - Dados: Um corpus de texto (ex.: livros do Projeto Gutenberg).\n",
    "#2 - Função de perda (Loss Function): Cross-entropy entre previsões e tokens reais.\n",
    "#3 - Backpropagation: Implementar gradientes manualmente (NumPy não tem autograd).\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True)) # Evita overflow\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    m = y_true.shape[0] #Número de exemplos\n",
    "    log_probs = -np.log(y_pred[range(m), y_true]) # Penaliza previsões erradas\n",
    "    return np.sum(log_probs) / m # Média do erro\n",
    "\n",
    "# Exemplo fictício:\n",
    "y_pred = softmax(np.random.randn(len(tokens), vocab_size))  # Previsões aleatórias\n",
    "y_true = np.array(tokens)  # Tokens reais\n",
    "loss = cross_entropy(y_pred, y_true)\n",
    "print(\"Loss:\", loss) # Quanto menor, melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68db9d-64e1-45c6-b598-d273e97238c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUNTANDO TODOS OS PASSOS E PREVENDO A PRÓXIMA LETRA\n",
    "import numpy as np\n",
    "\n",
    "# Configurações\n",
    "texto = \"ola mundo\"\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "token_para_id = {c: i for i, c in enumerate(vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate(vocabulario)}\n",
    "d_model = 16  # Dimensão dos embeddings\n",
    "\n",
    "# 1. Tokenização (input do usuário)\n",
    "input_texto = \"ola mun\"  # Tente prever o próximo caractere\n",
    "tokens = [token_para_id[c] for c in input_texto]\n",
    "\n",
    "# 2. Embeddings (aleatórios, pois não treinamos)\n",
    "embedding = np.random.randn(len(vocabulario), d_model)\n",
    "token_embeddings = embedding[tokens]  # Shape: [len(tokens), d_model]\n",
    "\n",
    "# 3. Self-Attention (simplificada)\n",
    "def attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return np.dot(weights, V)\n",
    "\n",
    "Q = token_embeddings\n",
    "K = token_embeddings\n",
    "V = token_embeddings\n",
    "saida_attention = attention(Q, K, V)\n",
    "\n",
    "# 4. Feed-Forward (aleatório)\n",
    "W1 = np.random.randn(d_model, 4 * d_model)\n",
    "b1 = np.random.randn(4 * d_model)\n",
    "W2 = np.random.randn(4 * d_model, d_model)\n",
    "b2 = np.random.randn(d_model)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "hidden = relu(np.dot(saida_attention[-1], W1) + b1)  # Pega o último token\n",
    "logits = np.dot(hidden, W2) + b2  # Shape: [d_model]\n",
    "\n",
    "# 5. Predição do próximo token\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits))  # Softmax\n",
    "proximo_token_id = np.argmax(probs)\n",
    "proximo_char = id_para_token[proximo_token_id]\n",
    "\n",
    "print(f\"Input: '{input_texto}' → Próximo caractere previsto: '{proximo_char}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167541-5ffd-4199-b6c3-ecb56a36cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12121a91-6bbd-4dab-a8e8-acc032dc94a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
