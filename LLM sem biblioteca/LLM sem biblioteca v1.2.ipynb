{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cff3a66-65a9-4c75-9c4c-561c14f2bf46",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "name 'W1' is used prior to global declaration (3429934536.py, line 93)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 93\u001b[1;36m\u001b[0m\n\u001b[1;33m    global W_Q, W_K, W_V, W1, b1, W2, b2, embedding\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m name 'W1' is used prior to global declaration\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Preparação dos Dados ---\n",
    "texto = \"ola mundo, como vai você?\"\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "token_para_id = {c: i for i, c in enumerate(vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate(vocabulario)}\n",
    "vocab_size = len(vocabulario)\n",
    "\n",
    "# Parâmetros\n",
    "seq_length = 5  # Tamanho da sequência de input\n",
    "d_model = 8     # Dimensão dos embeddings\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "# Criar pares (input, target)\n",
    "inputs, targets = [], []\n",
    "for i in range(len(texto) - seq_length):\n",
    "    inputs.append([token_para_id[c] for c in texto[i:i + seq_length]])\n",
    "    targets.append(token_para_id[texto[i + seq_length]])\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# --- 2. Inicialização do Modelo ---\n",
    "np.random.seed(42)\n",
    "embedding = np.random.randn(vocab_size, d_model) * 0.1\n",
    "\n",
    "# Self-Attention\n",
    "W_Q = np.random.randn(d_model, d_model) * 0.1\n",
    "W_K = np.random.randn(d_model, d_model) * 0.1\n",
    "W_V = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "# MLP\n",
    "W1 = np.random.randn(d_model, 4 * d_model) * 0.1\n",
    "b1 = np.zeros(4 * d_model)\n",
    "W2 = np.random.randn(4 * d_model, vocab_size) * 0.1\n",
    "b2 = np.zeros(vocab_size)\n",
    "\n",
    "# --- 3. Funções do Modelo ---\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def forward(x):\n",
    "    # Embeddings\n",
    "    x_embed = embedding[x]  # [seq_length, d_model]\n",
    "    \n",
    "    # Self-Attention\n",
    "    Q = np.dot(x_embed, W_Q)\n",
    "    K = np.dot(x_embed, W_K)\n",
    "    V = np.dot(x_embed, W_V)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_model)\n",
    "    attn_weights = softmax(scores)\n",
    "    attn_output = np.dot(attn_weights, V)  # [seq_length, d_model]\n",
    "    \n",
    "    # MLP (último token)\n",
    "    last_token = attn_output[-1]\n",
    "    hidden = np.maximum(0, np.dot(last_token, W1) + b1)  # ReLU\n",
    "    logits = np.dot(hidden, W2) + b2  # [vocab_size]\n",
    "    return logits, (x_embed, Q, K, V, attn_weights, attn_output, last_token, hidden)\n",
    "\n",
    "def compute_loss(logits, target):\n",
    "    probs = softmax(logits)\n",
    "    return -np.log(probs[target])\n",
    "\n",
    "# --- 4. Backpropagation Manual ---\n",
    "def backward(x, target, cache):\n",
    "    x_embed, Q, K, V, attn_weights, attn_output, last_token, hidden = cache\n",
    "    \n",
    "    # Gradiente da cross-entropy\n",
    "    probs = softmax(np.dot(hidden, W2) + b2)\n",
    "    d_logits = probs\n",
    "    d_logits[target] -= 1\n",
    "    \n",
    "    # Gradiente do MLP\n",
    "    d_W2 = np.outer(hidden, d_logits)\n",
    "    d_b2 = d_logits.copy()\n",
    "    d_hidden = np.dot(W2, d_logits)\n",
    "    d_hidden[hidden <= 0] = 0  # ReLU gradient\n",
    "    \n",
    "    d_W1 = np.outer(last_token, d_hidden)\n",
    "    d_b1 = d_hidden.copy()\n",
    "    d_last_token = np.dot(W1, d_hidden)\n",
    "    \n",
    "    # Gradiente da atenção (simplificado)\n",
    "    d_attn_output = np.zeros_like(attn_output)\n",
    "    d_attn_output[-1] = d_last_token\n",
    "    \n",
    "    d_V = np.dot(attn_weights.T, d_attn_output)\n",
    "    d_attn_weights = np.dot(d_attn_output, V.T)\n",
    "    \n",
    "    # Atualização dos pesos\n",
    "    global W_Q, W_K, W_V, W1, b1, W2, b2, embedding\n",
    "    W2 -= learning_rate * d_W2\n",
    "    b2 -= learning_rate * d_b2\n",
    "    W1 -= learning_rate * d_W1\n",
    "    b1 -= learning_rate * d_b1\n",
    "    \n",
    "    # (Otimização: gradientes de Q, K, V e embedding omitidos por simplicidade)\n",
    "\n",
    "# --- 5. Treinamento ---\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        logits, cache = forward(x)\n",
    "        loss = compute_loss(logits, target)\n",
    "        backward(x, target, cache)\n",
    "        \n",
    "        total_loss += loss\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(inputs)}\")\n",
    "\n",
    "# --- 6. Geração de Texto ---\n",
    "def generate_text(start_seq, max_length=10):\n",
    "    tokens = [token_para_id[c] for c in start_seq]\n",
    "    for _ in range(max_length):\n",
    "        logits, _ = forward(tokens[-seq_length:])\n",
    "        next_token = np.argmax(logits)\n",
    "        tokens.append(next_token)\n",
    "    return ''.join([id_para_token[t] for t in tokens])\n",
    "\n",
    "# Teste\n",
    "print(\"\\nTexto gerado:\", generate_text(\"ola m\", max_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6f7ea-6727-445e-821d-e61a4981c79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9999d-0432-47ee-88af-a25232dc11a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13432d-a776-4635-b804-7755f620159d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9dd6a-a700-48ad-be1f-644abad6cacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
