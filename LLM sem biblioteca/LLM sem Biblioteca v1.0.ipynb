{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61655b7f-24ee-47b7-a4b1-20a5c95a24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\nEste', '\\nUse', '(@#%)', '(á,', '1:', 'DOCUMENTO', 'Introdução', 'MOCK\\nCapítulo', 'Você.', 'acentos', 'com', 'e', 'embeddings!\\nAutor:', 'fictício', 'para', 'seu', 'símbolos', 'testar', 'texto', 'tokenizador', 'um', 'ã,', 'ç)', 'é', 'é,']\n"
     ]
    }
   ],
   "source": [
    "#PASSO 1: Tokenização\n",
    "#Aqui a gente transforma o texto em números\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Aqui abaixo na variável texto precisa fazer a injeção de textos das fontes que quiser.\n",
    "texto = \"DOCUMENTO MOCK\\nCapítulo 1: Introdução  \\nEste é um texto fictício com acentos (á, é, ã, ç) e símbolos (@#%) \\nUse para testar seu tokenizador e embeddings!\\nAutor: Você.\"\n",
    "texto = texto.split(' ')\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "print(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3450f1-e0e3-44e2-8a17-c5533cc3cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '\\nEste': 1, '\\nUse': 2, '(@#%)': 3, '(á,': 4, '1:': 5, 'DOCUMENTO': 6, 'Introdução': 7, 'MOCK\\nCapítulo': 8, 'Você.': 9, 'acentos': 10, 'com': 11, 'e': 12, 'embeddings!\\nAutor:': 13, 'fictício': 14, 'para': 15, 'seu': 16, 'símbolos': 17, 'testar': 18, 'texto': 19, 'tokenizador': 20, 'um': 21, 'ã,': 22, 'ç)': 23, 'é': 24, 'é,': 25}\n",
      "{0: '', 1: '\\nEste', 2: '\\nUse', 3: '(@#%)', 4: '(á,', 5: '1:', 6: 'DOCUMENTO', 7: 'Introdução', 8: 'MOCK\\nCapítulo', 9: 'Você.', 10: 'acentos', 11: 'com', 12: 'e', 13: 'embeddings!\\nAutor:', 14: 'fictício', 15: 'para', 16: 'seu', 17: 'símbolos', 18: 'testar', 19: 'texto', 20: 'tokenizador', 21: 'um', 22: 'ã,', 23: 'ç)', 24: 'é', 25: 'é,'}\n"
     ]
    }
   ],
   "source": [
    "#Seta cada palavra a um token e vice versa\n",
    "token_para_id = {c: i for i, c in enumerate (vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate (vocabulario)}\n",
    "print(token_para_id)\n",
    "print(id_para_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31967980-37d4-479b-9da5-df6752369b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [6, 8, 5, 7, 0, 1, 24, 21, 19, 14, 11, 10, 4, 25, 22, 23, 12, 17, 3, 2, 15, 18, 16, 20, 12, 13, 9]\n"
     ]
    }
   ],
   "source": [
    "#Tokenização\n",
    "tokens = [token_para_id[c] for c in texto] #Reescreve o texto inicial como tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32a255c-6e44-4f07-a6cb-32fd6087486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48866214  0.98030233  1.44460472 ... -0.92668625  1.03506295\n",
      "   1.1978157 ]\n",
      " [-0.29382744 -0.32905208  0.31011814 ...  0.58765784  0.93255109\n",
      "  -0.62690546]\n",
      " [-0.61953972 -0.23887039  0.43482099 ... -0.23239204 -1.05397606\n",
      "  -0.76669491]\n",
      " ...\n",
      " [-0.87887742 -1.83396644 -1.27937437 ... -1.52764746  0.65789263\n",
      "   0.68749182]\n",
      " [ 1.13119758 -0.46751032  1.42776106 ... -1.28724562  1.08805122\n",
      "  -0.37854147]\n",
      " [-0.4021161  -0.23834376 -0.12063326 ...  0.32063256 -0.51398253\n",
      "  -0.3826067 ]]\n"
     ]
    }
   ],
   "source": [
    "#PASSO 2: Embedding (Vetores de Palavras)\n",
    "#Cada token vira um vetor de dimensão \"d_model\"\n",
    "\n",
    "d_model = 256 #Isso é o tanto de números que vão representar uma letra! (Sim, precisa ser um número razoavelmente alto porque precisa diferenciar acentuações, maiúsculas, minúsculas e todo tipo de variação que existir de um caractere para outro)\n",
    "vocab_size = len(vocabulario)\n",
    "\n",
    "#Embeddings aleatórios (normalmente aprendidos durante o treino)\n",
    "embedding = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "#Transforma tokens em vetores\n",
    "token_embeddings = embedding[tokens] #Shape: [len(tokens), d_model]\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbc4809-05ce-4b6d-95c2-e46839a7a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASSO 3: Mecanismo de Atenção (Self-Attention)\n",
    "#Calcula a relação entre tokens, versão simplificada:\n",
    "def attention(Q, K, V):\n",
    "    # Q, K, V são matrizes de consulta, chave e valor\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Similaridade: Calcula o \"quão bem\" a palavra Q se relaciona com as outras (K)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax: Transforma os scores em probabilidades (soma = 1)\n",
    "    return np.dot(weights, V)  # Combinação: Combina os embeddings das palavras (V) usando os pesos de atenção\n",
    "\n",
    "# Exemplo:\n",
    "Q = token_embeddings  # Consulta: O que cada palavra está buscando\n",
    "K = token_embeddings  # Chave: Como cada palavra se descreve\n",
    "V = token_embeddings  # Valor: Informação real de cada palavra\n",
    "saida_attention = attention(Q, K, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12dfacb3-3237-4cd5-9ca0-837277ecd6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48865919  0.98029643  1.44459362 ... -0.92668003  1.03505686\n",
      "   1.19781057]\n",
      " [-0.29376997 -0.32894462  0.31008805 ...  0.58761112  0.93246826\n",
      "  -0.62687289]\n",
      " [-0.61953837 -0.23886899  0.43482038 ... -0.23239149 -1.05397395\n",
      "  -0.76669356]\n",
      " ...\n",
      " [-0.87887662 -1.83396444 -1.27937319 ... -1.5276455   0.65789216\n",
      "   0.68749164]\n",
      " [ 1.13119651 -0.46750938  1.42775959 ... -1.28724406  1.08805013\n",
      "  -0.37854058]\n",
      " [-0.40211205 -0.23833944 -0.12063269 ...  0.32063204 -0.51397904\n",
      "  -0.38260605]]\n"
     ]
    }
   ],
   "source": [
    "#PASSO 4: Rede Neural Feed-Forward\n",
    "#Uma MLP simples para processar os vetores após o mecanismo de atenção\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    hidden = np.maximum(0, np.dot(x, W1) + b1)  # Camada oculta com ReLU\n",
    "    return np.dot(hidden, W2) + b2 # Camada de saída\n",
    "\n",
    "# Pesos aleatórios (em um modelo real, seriam aprendidos)\n",
    "W1 = np.random.randn(d_model, 4 * d_model) # Pesos da camada oculta\n",
    "b1 = np.random.randn(4 * d_model) # Vieses da camada oculta\n",
    "W2 = np.random.randn(4 * d_model, d_model) # Pesos da camada de saída\n",
    "b2 = np.random.randn(d_model) # Vieses da camada de saída\n",
    "\n",
    "saida_ff = feed_forward(saida_attention, W1, b1, W2, b2)\n",
    "\n",
    "print(saida_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e35691f-0e41-418e-8a66-07a17b838492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.854085529317129\n"
     ]
    }
   ],
   "source": [
    "#PASSO 5: Treinamento\n",
    "#Para treinar precisaríamos\n",
    "#1 - Dados: Um corpus de texto (ex.: livros do Projeto Gutenberg).\n",
    "#2 - Função de perda (Loss Function): Cross-entropy entre previsões e tokens reais.\n",
    "#3 - Backpropagation: Implementar gradientes manualmente (NumPy não tem autograd).\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True)) # Evita overflow\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    m = y_true.shape[0] #Número de exemplos\n",
    "    log_probs = -np.log(y_pred[range(m), y_true]) # Penaliza previsões erradas\n",
    "    return np.sum(log_probs) / m # Média do erro\n",
    "\n",
    "# Exemplo fictício:\n",
    "y_pred = softmax(np.random.randn(len(tokens), vocab_size))  # Previsões aleatórias\n",
    "y_true = np.array(tokens)  # Tokens reais\n",
    "loss = cross_entropy(y_pred, y_true)\n",
    "print(\"Loss:\", loss) # Quanto menor, melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f1f76-213d-4234-9641-31de96568c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e12aa-fafa-4705-a9fc-3e579bdf3777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c380bf-de3f-496d-be76-057623ccab81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e2ccecc-5ac7-4b98-9128-5d40cf5262ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: O rato roeu a roupa do rei de roma.\n",
      "Resposta gerada: rei\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === PASSO 1: Tokenização ===\n",
    "texto = \"O rato roeu a roupa do rei de roma.\"\n",
    "palavras = texto.split()  # Divide o texto em palavras\n",
    "vocabulario = sorted(list(set(palavras)))  # Vocabulário único\n",
    "\n",
    "# Mapeamento palavra → ID e vice-versa\n",
    "token_para_id = {palavra: i for i, palavra in enumerate(vocabulario)}\n",
    "id_para_token = {i: palavra for i, palavra in enumerate(vocabulario)}\n",
    "\n",
    "# Tokeniza o texto\n",
    "tokens = [token_para_id[palavra] for palavra in palavras]\n",
    "\n",
    "# === PASSO 2: Embeddings ===\n",
    "d_model = 64  # Dimensão dos vetores\n",
    "vocab_size = len(vocabulario)\n",
    "\n",
    "# Cria a embedding_matrix (aleatória inicialmente)\n",
    "embedding_matrix = np.random.randn(vocab_size, d_model)  # <--- AGORA DEFINIDA!\n",
    "\n",
    "# Obtém os embeddings das palavras do texto\n",
    "token_embeddings = embedding_matrix[tokens]  # Shape: [n_palavras, d_model]\n",
    "\n",
    "# === PASSO 3: Self-Attention ===\n",
    "def attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return np.dot(weights, V)\n",
    "\n",
    "saida_attention = attention(token_embeddings, token_embeddings, token_embeddings)\n",
    "\n",
    "# === PASSO 4: Feed-Forward ===\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    hidden = np.maximum(0, np.dot(x, W1) + b1)  # ReLU\n",
    "    return np.dot(hidden, W2) + b2\n",
    "\n",
    "# Inicializa pesos\n",
    "W1 = np.random.randn(d_model, 4 * d_model)\n",
    "b1 = np.random.randn(4 * d_model)\n",
    "W2 = np.random.randn(4 * d_model, d_model)\n",
    "b2 = np.random.randn(d_model)\n",
    "\n",
    "saida_ff = feed_forward(saida_attention, W1, b1, W2, b2)\n",
    "\n",
    "# === PASSO 5: Gerar Resposta ===\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Pega o último vetor (representação da frase toda)\n",
    "vetor_frase = saida_ff[-1]\n",
    "\n",
    "# Calcula probabilidades para todas as palavras do vocabulário\n",
    "logits = np.dot(vetor_frase, embedding_matrix.T)\n",
    "probs = softmax(logits)\n",
    "\n",
    "# Escolhe a palavra mais provável como resposta\n",
    "resposta_id = np.argmax(probs)\n",
    "resposta = id_para_token[resposta_id]\n",
    "\n",
    "print(f\"Texto original: {texto}\")\n",
    "print(f\"Resposta gerada: {resposta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e927df-36fc-49a0-b219-ff374a9951f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
