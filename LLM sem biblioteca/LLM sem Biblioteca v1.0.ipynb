{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61655b7f-24ee-47b7-a4b1-20a5c95a24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\nEste', '\\nUse', '(@#%)', '(á,', '1:', 'DOCUMENTO', 'Introdução', 'MOCK\\nCapítulo', 'Você.', 'acentos', 'com', 'e', 'embeddings!\\nAutor:', 'fictício', 'para', 'seu', 'símbolos', 'testar', 'texto', 'tokenizador', 'um', 'ã,', 'ç)', 'é', 'é,']\n"
     ]
    }
   ],
   "source": [
    "#PASSO 1: Tokenização\n",
    "#Aqui a gente transforma o texto em números\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Aqui abaixo na variável texto precisa fazer a injeção de textos das fontes que quiser.\n",
    "texto = \"DOCUMENTO MOCK\\nCapítulo 1: Introdução  \\nEste é um texto fictício com acentos (á, é, ã, ç) e símbolos (@#%) \\nUse para testar seu tokenizador e embeddings!\\nAutor: Você.\"\n",
    "texto = texto.split(' ')\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "print(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c3450f1-e0e3-44e2-8a17-c5533cc3cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, '\\nEste': 1, '\\nUse': 2, '(@#%)': 3, '(á,': 4, '1:': 5, 'DOCUMENTO': 6, 'Introdução': 7, 'MOCK\\nCapítulo': 8, 'Você.': 9, 'acentos': 10, 'com': 11, 'e': 12, 'embeddings!\\nAutor:': 13, 'fictício': 14, 'para': 15, 'seu': 16, 'símbolos': 17, 'testar': 18, 'texto': 19, 'tokenizador': 20, 'um': 21, 'ã,': 22, 'ç)': 23, 'é': 24, 'é,': 25}\n",
      "{0: '', 1: '\\nEste', 2: '\\nUse', 3: '(@#%)', 4: '(á,', 5: '1:', 6: 'DOCUMENTO', 7: 'Introdução', 8: 'MOCK\\nCapítulo', 9: 'Você.', 10: 'acentos', 11: 'com', 12: 'e', 13: 'embeddings!\\nAutor:', 14: 'fictício', 15: 'para', 16: 'seu', 17: 'símbolos', 18: 'testar', 19: 'texto', 20: 'tokenizador', 21: 'um', 22: 'ã,', 23: 'ç)', 24: 'é', 25: 'é,'}\n"
     ]
    }
   ],
   "source": [
    "#Seta cada palavra a um token e vice versa\n",
    "token_para_id = {c: i for i, c in enumerate (vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate (vocabulario)}\n",
    "print(token_para_id)\n",
    "print(id_para_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31967980-37d4-479b-9da5-df6752369b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [6, 8, 5, 7, 0, 1, 24, 21, 19, 14, 11, 10, 4, 25, 22, 23, 12, 17, 3, 2, 15, 18, 16, 20, 12, 13, 9]\n"
     ]
    }
   ],
   "source": [
    "#Tokenização\n",
    "tokens = [token_para_id[c] for c in texto] #Reescreve o texto inicial como tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e32a255c-6e44-4f07-a6cb-32fd6087486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01786658  1.91531678 -0.84834873 ...  2.49264046 -1.91417195\n",
      "  -0.54391071]\n",
      " [-0.16182209  0.49858315  2.23865579 ... -2.71723215  1.21656818\n",
      "  -0.37567252]\n",
      " [-0.83725326  1.04301379  1.03118608 ... -0.57616258 -1.24966438\n",
      "   0.35365948]\n",
      " ...\n",
      " [-1.1016822   1.31416071 -0.67419309 ...  1.68009913 -0.36395967\n",
      "   0.42462353]\n",
      " [-0.0525345  -0.59194368 -0.60421231 ...  1.15227626  0.07397107\n",
      "  -1.80672398]\n",
      " [ 0.02892368 -0.02983188  0.9794713  ... -1.66929551 -0.25618648\n",
      "   1.19562879]]\n"
     ]
    }
   ],
   "source": [
    "#PASSO 2: Embedding (Vetores de Palavras)\n",
    "#Cada token vira um vetor de dimensão \"d_model\"\n",
    "\n",
    "d_model = 256 #Isso é o tanto de números que vão representar uma letra! (Sim, precisa ser um número razoavelmente alto porque precisa diferenciar acentuações, maiúsculas, minúsculas e todo tipo de variação que existir de um caractere para outro)\n",
    "vocab_size = len(vocabulario)\n",
    "\n",
    "#Embeddings aleatórios (normalmente aprendidos durante o treino)\n",
    "embedding = np.random.randn(vocab_size, d_model)\n",
    "\n",
    "#Transforma tokens em vetores\n",
    "token_embeddings = embedding[tokens] #Shape: [len(tokens), d_model]\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fbc4809-05ce-4b6d-95c2-e46839a7a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASSO 3: Mecanismo de Atenção (Self-Attention)\n",
    "#Calcula a relação entre tokens, versão simplificada:\n",
    "def attention(Q, K, V):\n",
    "    # Q, K, V são matrizes de consulta, chave e valor\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Similaridade: Calcula o \"quão bem\" a palavra Q se relaciona com as outras (K)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax: Transforma os scores em probabilidades (soma = 1)\n",
    "    return np.dot(weights, V)  # Combinação: Combina os embeddings das palavras (V) usando os pesos de atenção\n",
    "\n",
    "# Exemplo:\n",
    "Q = token_embeddings  # Consulta: O que cada palavra está buscando\n",
    "K = token_embeddings  # Chave: Como cada palavra se descreve\n",
    "V = token_embeddings  # Valor: Informação real de cada palavra\n",
    "saida_attention = attention(Q, K, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12dfacb3-3237-4cd5-9ca0-837277ecd6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01786511  1.9153097  -0.84834456 ...  2.49263048 -1.91416282\n",
      "  -0.54390648]\n",
      " [-0.16181902  0.49858038  2.23864546 ... -2.71721724  1.21656102\n",
      "  -0.37566793]\n",
      " [-0.83724962  1.04301106  1.03118314 ... -0.57616014 -1.24965876\n",
      "   0.35365962]\n",
      " ...\n",
      " [-1.10167998  1.31415928 -0.67419174 ...  1.68009728 -0.36395892\n",
      "   0.42462308]\n",
      " [-0.05252838 -0.59193978 -0.60420591 ...  1.15226988  0.07397181\n",
      "  -1.80670164]\n",
      " [ 0.02892589 -0.02982777  0.97946472 ... -1.66927893 -0.25618466\n",
      "   1.1956216 ]]\n"
     ]
    }
   ],
   "source": [
    "#PASSO 4: Rede Neural Feed-Forward\n",
    "#Uma MLP simples para processar os vetores após o mecanismo de atenção\n",
    "def feed_forward(x, W1, b1, W2, b2):\n",
    "    hidden = np.maximum(0, np.dot(x, W1) + b1)  # Camada oculta com ReLU\n",
    "    return np.dot(hidden, W2) + b2 # Camada de saída\n",
    "\n",
    "# Pesos aleatórios (em um modelo real, seriam aprendidos)\n",
    "W1 = np.random.randn(d_model, 4 * d_model) # Pesos da camada oculta\n",
    "b1 = np.random.randn(4 * d_model) # Vieses da camada oculta\n",
    "W2 = np.random.randn(4 * d_model, d_model) # Pesos da camada de saída\n",
    "b2 = np.random.randn(d_model) # Vieses da camada de saída\n",
    "\n",
    "saida_ff = feed_forward(saida_attention, W1, b1, W2, b2)\n",
    "\n",
    "print(saida_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e35691f-0e41-418e-8a66-07a17b838492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.653326188796029\n"
     ]
    }
   ],
   "source": [
    "#PASSO 5: Treinamento\n",
    "#Para treinar precisaríamos\n",
    "#1 - Dados: Um corpus de texto (ex.: livros do Projeto Gutenberg).\n",
    "#2 - Função de perda (Loss Function): Cross-entropy entre previsões e tokens reais.\n",
    "#3 - Backpropagation: Implementar gradientes manualmente (NumPy não tem autograd).\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True)) # Evita overflow\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    m = y_true.shape[0] #Número de exemplos\n",
    "    log_probs = -np.log(y_pred[range(m), y_true]) # Penaliza previsões erradas\n",
    "    return np.sum(log_probs) / m # Média do erro\n",
    "\n",
    "# Exemplo fictício:\n",
    "y_pred = softmax(np.random.randn(len(tokens), vocab_size))  # Previsões aleatórias\n",
    "y_true = np.array(tokens)  # Tokens reais\n",
    "loss = cross_entropy(y_pred, y_true)\n",
    "print(\"Loss:\", loss) # Quanto menor, melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68db9d-64e1-45c6-b598-d273e97238c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUNTANDO TODOS OS PASSOS E PREVENDO A PRÓXIMA LETRA\n",
    "import numpy as np\n",
    "\n",
    "# Configurações\n",
    "texto = \"ola mundo\"\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "token_para_id = {c: i for i, c in enumerate(vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate(vocabulario)}\n",
    "d_model = 16  # Dimensão dos embeddings\n",
    "\n",
    "# 1. Tokenização (input do usuário)\n",
    "input_texto = \"ola mun\"  # Tente prever o próximo caractere\n",
    "tokens = [token_para_id[c] for c in input_texto]\n",
    "\n",
    "# 2. Embeddings (aleatórios, pois não treinamos)\n",
    "embedding = np.random.randn(len(vocabulario), d_model)\n",
    "token_embeddings = embedding[tokens]  # Shape: [len(tokens), d_model]\n",
    "\n",
    "# 3. Self-Attention (simplificada)\n",
    "def attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return np.dot(weights, V)\n",
    "\n",
    "Q = token_embeddings\n",
    "K = token_embeddings\n",
    "V = token_embeddings\n",
    "saida_attention = attention(Q, K, V)\n",
    "\n",
    "# 4. Feed-Forward (aleatório)\n",
    "W1 = np.random.randn(d_model, 4 * d_model)\n",
    "b1 = np.random.randn(4 * d_model)\n",
    "W2 = np.random.randn(4 * d_model, d_model)\n",
    "b2 = np.random.randn(d_model)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "hidden = relu(np.dot(saida_attention[-1], W1) + b1)  # Pega o último token\n",
    "logits = np.dot(hidden, W2) + b2  # Shape: [d_model]\n",
    "\n",
    "# 5. Predição do próximo token\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits))  # Softmax\n",
    "proximo_token_id = np.argmax(probs)\n",
    "proximo_char = id_para_token[proximo_token_id]\n",
    "\n",
    "print(f\"Input: '{input_texto}' → Próximo caractere previsto: '{proximo_char}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167541-5ffd-4199-b6c3-ecb56a36cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12121a91-6bbd-4dab-a8e8-acc032dc94a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
