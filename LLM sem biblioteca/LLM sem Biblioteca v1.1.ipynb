{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb629d0-b43b-4eb7-bddc-9a143754b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 1: Preparação dos Dados\n",
    "#Vamos usar um corpus de texto simples (ex.: \"ola mundo, como vai você?\").\n",
    "#Tokenização: Caracteres individuais.\n",
    "#Inputs/Saídas: Para cada sequência de N caracteres, prever o próximo.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Dataset de exemplo\n",
    "texto = \"ola mundo, como vai você?\"\n",
    "vocabulario = sorted(list(set(texto)))\n",
    "token_para_id = {c: i for i, c in enumerate(vocabulario)}\n",
    "id_para_token = {i: c for i, c in enumerate(vocabulario)}\n",
    "vocab_size = len(vocabulario)\n",
    "\n",
    "# Parâmetros\n",
    "seq_length = 5  # Tamanho da sequência de input\n",
    "d_model = 8     # Dimensão dos embeddings (reduzida para simplificar)\n",
    "\n",
    "# Criar pares (input, target)\n",
    "inputs, targets = [], []\n",
    "for i in range(len(texto) - seq_length):\n",
    "    input_seq = texto[i:i + seq_length]\n",
    "    target_char = texto[i + seq_length]\n",
    "    inputs.append([token_para_id[c] for c in input_seq])\n",
    "    targets.append(token_para_id[target_char])\n",
    "\n",
    "inputs = np.array(inputs)  # Shape: [n_exemplos, seq_length]\n",
    "targets = np.array(targets)  # Shape: [n_exemplos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c531253-3b04-4683-8b68-1feb8d49cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 2: Inicialização do Modelo\n",
    "#Definimos:\n",
    "#Embeddings: vocab_size x d_model.\n",
    "#Pesos da Self-Attention: Q, K, V.\n",
    "#MLP: 1 camada oculta.\n",
    "# Embeddings (aleatórios, mas normalizados)\n",
    "embedding = np.random.randn(vocab_size, d_model) * 0.1\n",
    "\n",
    "# Self-Attention (projetores Q, K, V)\n",
    "W_Q = np.random.randn(d_model, d_model) * 0.1\n",
    "W_K = np.random.randn(d_model, d_model) * 0.1\n",
    "W_V = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "# MLP (1 camada oculta)\n",
    "W1 = np.random.randn(d_model, 4 * d_model) * 0.1\n",
    "b1 = np.random.randn(4 * d_model) * 0.1\n",
    "W2 = np.random.randn(4 * d_model, vocab_size) * 0.1\n",
    "b2 = np.random.randn(vocab_size) * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd646b0-e154-4b30-b3e6-e14e76f52a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [-0.08157956 -0.05113903 -0.0379167   0.00635051  0.0763397  -0.06914389\n",
      "  0.06945129  0.11150198 -0.01258988  0.11689556  0.02346421  0.068696\n",
      "  0.12067095  0.26868934]\n"
     ]
    }
   ],
   "source": [
    "#Passo 3: Forward Pass\n",
    "#Calculamos a saída do modelo para um input (com atenção simplificada).\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def forward(x):\n",
    "    # x: sequência de tokens (ex.: [5, 3, 1, 0, 2])\n",
    "    # 1. Embeddings\n",
    "    x_embed = embedding[x]  # Shape: [seq_length, d_model]\n",
    "    \n",
    "    # 2. Self-Attention (single head)\n",
    "    Q = np.dot(x_embed, W_Q)  # [seq_length, d_model]\n",
    "    K = np.dot(x_embed, W_K)\n",
    "    V = np.dot(x_embed, W_V)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_model)\n",
    "    attn_weights = softmax(scores)\n",
    "    attn_output = np.dot(attn_weights, V)  # [seq_length, d_model]\n",
    "    \n",
    "    # 3. MLP (apenas no último token)\n",
    "    last_token = attn_output[-1]  # Pega o último token\n",
    "    hidden = np.maximum(0, np.dot(last_token, W1) + b1)  # ReLU\n",
    "    logits = np.dot(hidden, W2) + b2  # [vocab_size]\n",
    "    return logits\n",
    "\n",
    "# Exemplo:\n",
    "logits = forward(inputs[0])\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef003092-8fe9-43ea-aa5a-3d8d5c279b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passo 4: Loss e Backpropagation\n",
    "#Usamos cross-entropy e calculamos os gradientes manualmente.\n",
    "\n",
    "def compute_loss(logits, target):\n",
    "    probs = softmax(logits)\n",
    "    loss = -np.log(probs[target])\n",
    "    return loss\n",
    "\n",
    "# Gradientes (via diferenças finitas ou derivadas analíticas)\n",
    "def backward(x, target, learning_rate=0.01):\n",
    "    # Forward pass (guardando valores intermediários)\n",
    "    x_embed = embedding[x]\n",
    "    Q = np.dot(x_embed, W_Q)\n",
    "    K = np.dot(x_embed, W_K)\n",
    "    V = np.dot(x_embed, W_V)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_model)\n",
    "    attn_weights = softmax(scores)\n",
    "    attn_output = np.dot(attn_weights, V)\n",
    "    last_token = attn_output[-1]\n",
    "    hidden = np.maximum(0, np.dot(last_token, W1) + b1)\n",
    "    logits = np.dot(hidden, W2) + b2\n",
    "    \n",
    "    # 1. Gradiente da cross-entropy\n",
    "    probs = softmax(logits)\n",
    "    d_logits = probs\n",
    "    d_logits[target] -= 1\n",
    "    \n",
    "    # 2. Gradiente do MLP\n",
    "    d_W2 = np.outer(hidden, d_logits)\n",
    "    d_b2 = d_logits.copy()\n",
    "    d_hidden = np.dot(W2, d_logits)\n",
    "    d_hidden[hidden <= 0] = 0  # Gradiente da ReLU\n",
    "    \n",
    "    d_W1 = np.outer(last_token, d_hidden)\n",
    "    d_b1 = d_hidden.copy()\n",
    "    \n",
    "    # 3. Gradiente da atenção (simplificado)\n",
    "    # (Implementação completa requer mais cuidado)\n",
    "    d_attn_output = np.dot(W1, d_hidden)  # [d_model]\n",
    "    \n",
    "    # Atualização dos pesos (SGD)\n",
    "    W2 -= learning_rate * d_W2\n",
    "    b2 -= learning_rate * d_b2\n",
    "    W1 -= learning_rate * d_W1\n",
    "    b1 -= learning_rate * d_b1\n",
    "    \n",
    "    # Gradiente dos embeddings (opcional)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "503d7fb3-b8c5-441c-93da-ba08a6bb2927",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m target \u001b[38;5;241m=\u001b[39m targets[i]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass (guardando cache para backprop)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m logits, cache \u001b[38;5;241m=\u001b[39m forward(x)  \u001b[38;5;66;03m# Agora forward retorna (logits, cache)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(logits, target)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m## Extrai variáveis do cache\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#Passo 5: Treinamento\n",
    "#Loop sobre os dados para ajustar os pesos.\n",
    "\n",
    "# --- 5. Treinamento ---\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Forward pass (guardando cache para backprop)\n",
    "        logits, cache = forward(x)  # Agora forward retorna (logits, cache)\n",
    "        loss = compute_loss(logits, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        ## Extrai variáveis do cache\n",
    "        x_embed, Q, K, V, attn_weights, attn_output, last_token, hidden = cache\n",
    "        \n",
    "        # Gradiente da cross-entropy\n",
    "        probs = softmax(logits)\n",
    "        d_logits = probs\n",
    "        d_logits[target] -= 1\n",
    "        \n",
    "        # Gradiente do MLP\n",
    "        d_W2 = np.outer(hidden, d_logits)\n",
    "        d_b2 = d_logits.copy()\n",
    "        d_hidden = np.dot(W2, d_logits)\n",
    "        d_hidden[hidden <= 0] = 0  # Gradiente da ReLU\n",
    "        \n",
    "        d_W1 = np.outer(last_token, d_hidden)\n",
    "        d_b1 = d_hidden.copy()\n",
    "        \n",
    "        # Atualização dos pesos\n",
    "        W2 -= learning_rate * d_W2\n",
    "        b2 -= learning_rate * d_b2\n",
    "        W1 -= learning_rate * d_W1\n",
    "        b1 -= learning_rate * d_b1\n",
    "        \n",
    "        total_loss += loss\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0702f0b8-a9ec-4596-9869-59fc91f0ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola miiiii\n"
     ]
    }
   ],
   "source": [
    "#Passo 6: Geração de Texto\n",
    "#Após o treino, use o modelo para prever caracteres.\n",
    "\n",
    "def generate_text(start_seq, max_length=10):\n",
    "    tokens = [token_para_id[c] for c in start_seq]\n",
    "    for _ in range(max_length):\n",
    "        logits = forward(tokens[-seq_length:])\n",
    "        next_token = np.argmax(logits)\n",
    "        tokens.append(next_token)\n",
    "    return ''.join([id_para_token[t] for t in tokens])\n",
    "\n",
    "# Exemplo:\n",
    "print(generate_text(\"ola m\", max_length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a17f8-7558-4982-828e-263add610da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
